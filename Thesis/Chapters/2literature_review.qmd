# Literature review

In service of enhancing LC-MS metabolomics data analysis, the literature review aims at elucidating how Notame, Bioconductor and Quarto promote quality and reproducibility. First, Notame and how it relates to research quality is addressed in an order which best explains central concepts. Notame is also contrasted with other approaches from the literature, providing context for the best practices and anticipating developments. It is important to note that a community-wide effort for best practices is being undertaken by the Metabolomics Quality Assurance & Quality Control Consortium (mQACC), although it is limited to quality assurance (QA) and quality control (QC) (Kirwan et al. 2022). The consensus practices recognized by mQACC are aligned with Notame. Best practices are not necessarily prescriptive, but serve as a guide to state-of-the-art. Accordingly, the Data Analysis Working Group (DAWG) of the Metabolomics Standards Initiative (MSI) has only acknowledged very broad best practices, such as the use of QC samples and cross-validation in supervised learning (Goodacre et al. 2020). For oversight, the metabolomics functionalities are categorized into data extraction, data pretreatment, feature selection and biological context (Figure 1). The focus on data pretreatment and feature selection, as these are catered to by the Notame R package.

Reporting standards are crucial for reproducibility irrespective of the analysis approach, and are disseminated by MSI. DAWG has set forth standards for reporting the data analysis (Goodacre et al. 2007), for which the utility of Bioconductor and Quarto will become apparent towards the end of the literature review.

![Figure 1. Overview of LC-MS metabolomics data analysis. Data extraction refers to steps leading to a quantitative dataset from data collection. Data pretreatment involves completing the dataset by way of reducing unwanted variation and data preparation dependent on downstream methods. Feature selection aims to select interesting metabolite species across study groups/time points. Functionality related to biological context elucidates the biological meaning of interesting metabolite species. Reproducibility is implied in the above steps and in research at large.](./Figures/literature_review_figure.png){width=100%}


## Data extraction

Data extraction results in a quantitative dataset, consisting of metabolite species characterized by their abundance across samples and metadata.

### Data collection

Metabolomics research requires expertise in analytical chemistry, biochemistry, bioinformatics and data analysis (Klåvus et al. 2020). In metabolomics research, biological samples are typically analysed using tandem liquid chromatography-mass spectrometry (LC-MS/MS) (Gika et al. 2019). In the LC-MS step, metabolites in the liquid phase are separated in a chromatography column and ionized for detection by their mass-to-charge ratios (m/z) (McMaster 2005, p. 52). In the LC-MS/MS step, precursor ions from the first stage are fragmented for detection with improved specificity (McMaster 2005, p. 1).

The above is for simplicity; a more nuanced understanding of LC-MS/MS involves chromatography columns, ionization, fragmentation, isotopes, ion suppression, neutral loss formation and detection. Several types of chromatography columns can be used for separation of the metabolites in LC/MS. Often, two columns are used to separate metabolites with different physico-chemical characteristics (McMaster 2005, p. 23). LC-MS/MS ionization methods can be divided into soft ionization and hard ionization. For the LC-MS step, soft ionization is used, commonly electrospray ionization (McMaster 2005, p. 53). Electrospray ionization is operated in positive ion mode and negative ion mode, resulting in protonation or deprotonation of the metabolite, respectively (Ardrey 2003, p. 106). As opposed to hard ionization, the low energy of soft ionization mostly preserves the structure of the metabolite, and fragmentation of the metabolite is limited (McMaster 2005, p. 53). Metabolite species are charged due to the addition or loss of atoms and electrons, which is facilitated by volatile additives such as formic acid and ammonium formate (McMaster 2005, p. 147). Common metabolite species, for example isotopes and those promoted by additives, are characterized by known mass differences with respect to the parent metabolite. Other metabolite species, such as adducts and fragment ions, form less predictably (Schug and McNair 2003) and introduce redundancy in the dataset (Klåvus et al. 2020). These also contribute to reduced detector response, so called ion suppression, which reduces sensitivity of detection (Erngren et al. 2019). Neutral loss formation, meaning metabolite species lost as neutral molecules, also reduces the sensitivity of detection (McMaster 2005, p. 95).

Charged metabolite species are then accelerated and deflected by a magnetic field for detection (McMaster 2005, p. 45), although the details depend on the instrumentation. The magnitude of deflection depends on the m/z of the metabolite species (McMaster 2005, p. 45). By manipulating the magnetic field, the stream of metabolite species characterized by differing m/z is nudged towards the detector (McMaster 2005, p. 45). As metabolite species hit the detector, an electric current is perturbed, which is amplified and recorded as intensity relative to m/z (Figure 2) (McMaster 2005, p. 62). The more ions arriving at the detector, the greater the perturbation and recorded intensity. In the LC-MS/MS step, a hard ionization technique like collision-induced dissociation is used to fragment metabolite species from the LC-MS step for improved specificity (McMaster 2005, p. 103).

### Preprocessing

There are several approaches to preprocessing of the signal to quantitate the metabolite species across samples, but the two most central steps are as follows. First, the spectrum consisting of intensity values relative to m/z undergoes algorithmic peak-picking (Figure 2) to quantitate the signal from each metabolite species (Shimadzu 2023). Second, the retention time (RT), or the time a metabolite takes to pass through the chromatography column, is calculated for each metabolite species based on max peak intensities (Shimadzu 2023) or the raw data from the LC-MS spectra (Figure 2). Finally, since the RT of each metabolite varies between samples because of chromatographic conditions, contamination and other factors, RTs are aligned to match metabolite species across samples (Shimadzu 2023). These preprocessing steps are often performed using point-and-click software, such as MS-DIAL (Tsugawa et al. 2015) specified by Notame. MS-DIAL accounts for isotopes, most common adducts and some fragment ions and combine their abundances into a single entry in the quantitative dataset (Tsugawa et al. 2015). MS-DIAL also computes RTs and performs retention time alignment (Tsugawa et al. 2015). The identities of the metabolite species in the quantitative dataset are characterized by their average m/z and RT values. Quantitative analysis is concerned with quantitative datasets from the LC-MS step extracted using different chromatographic modes. RT and LC-MS/MS spectra are used in metabolite identification as per the most robust identification level specified by the Metabolomics Standards Initiative (Sumer et al. 2007).


![Figure 2. LC-MS data overview. The detector records chromatographically separated metabolite species' intensity relative to their m/z, which are integrated as separate peaks in the peak-picking phase of preprocessing. RT of peaks is determined from the maximum peak intensity. Finally, the same metabolite species across samples is identified with retention time alignment (not shown)](./Figures/LCMS_figure.png){width=100%}

### Quality assurance

QA and QC can be thought of as complementary, where QA refers to the steps to assure the quality of the data irrespective of whether it's effect can be reported or not, whereas QC is concerned with, well, control of the quality by flagging low-quality metabolite species (Broadhurst et al. 2018). A useful perspective on QA is that it aims to minimize unwanted experimental variation from sample preparation and data collection, instilling confidence in the results. Unwanted variation can be conceptualized as consisting of measurement error, unwanted experimental variation and unwanted biological variation. Variation is often represented as variance, the average of squared deviations from the mean for each study group/time point.

Untargeted LC-MS, in particular, suffers from scant standardization of QA (Broadhurst et al. 2018). One reason is the difficulty in using isotope-labeled internal standards for QA in untargeted approaches aiming to maximize the number and physico-chemical diversity of metabolites detected in a biological sample. As such, QA practices can't be directly adopted from targeted approaches (Broadhurst et al. 2018). Standardized QA has not been advanced by MSI or mQACC. However, mQACC recognizes a consensus on QA for which reporting standards have been disseminated. Next, the widely cited QA practices from Broadhurst et al. (2018), in line with those recognized by mQACC and Notame, are presented from the perspective of QC samples and data collection, as these constitute important context for data pretreatment. Biological sample preparation for LC-MS metabolomics experiments is minimal and not critical for the remainder of the thesis, and is thus omitted.

QC samples have extensive utility in assuring the quality of untargeted metabolomics experiments, especially in normalization and estimating precision. To account for experimental variation in QC samples, it is critical to process the QC samples and biological samples identically. One way to prepare QC samples is to aliquot each biological sample into replicates, and randomize the order of injection. This greatly increases data collection and QC time because of multiplying the number of samples. A more time efficient alternative is to aliquot a single biological sample into > 5 replicates, which are evenly distributed in the batch. Here, the assumption that the chosen, aliquoted biological sample has a representative metabolite profile is problematic since there may be metabolites present in other biological samples which are below the detection threshold in the sample from which replicates are aliquoted. Pooled QC samples are a workable compromise and used in Notame, where aliquots from each biological sample are pooled, subaliquoted and distributed evenly in the injection sequence.

Long-term reference QC samples and standard reference materials can be used for QA across batches. Long-term reference QC samples for studies involving the same experimental procedure are simply large pooled QC samples, aliquoted and stored in liquid nitrogen. Including such samples in each batch allows for estimating the precision in and across batches. Standard reference materials, on the other hand, are suitable for QA across laboratories as they are within strict tolerances.

Another aspect of QA is assessing system suitability to reduce unwanted experimental variation. At the very beginning of an experiment, a blank excluding everything but solvents is injected. This provides an assessment of possible contaminants in the system, necessitating cleaning or column change. If the blank doesn't show any unexpected peaks, several QC samples are injected for conditioning, that is reducing absorption of metabolites in the system, RT variability and stabilizing the detector response. A process blank is injected in the middle of the conditioning QC sample sequence to identify signals from contamination, biochemical precipitation, extraction and more. The process blank is prepared using the exact same protocol as in preparing the biological samples, but excluding the biological sample. When the signal from the conditioning QC samples has stabilized, two QC samples are injected, marking the start of the experimental run. Before the first biological sample, a standard reference or long-term reference QC sample may be run to to estimate accuracy and precision across batches, followed by two more interspersed later in the injection sequence. Biological samples are then injected, interspersed by QC samples every five samples, for example. At the end of the injection sequence, two QC samples are injected so that the regression model used to normalize for drift interpolates to the last biological samples. Then, another process blank is injected to test for cumulative carryover, that is the detection of biological signals which "leak" to the next sample, which can inform adjusting the washing between sample injections. Peaks in the processing blank satisfying specific exclusion criteria are removed from the quantitative dataset. Finally, five or so QC samples are included for LC-MS/MS data collection. The number of processing blanks, reference samples, sequential QC samples, LC-MS/MS samples and the ratio of QC samples to biological samples should be adjusted for the experiment and instrumentation at hand.

Despite QA, some unwanted variation makes it to the quantitative dataset. Some adducts and fragment ions form unpredictably and can be regarded as introducing noise (Schug and McNair 2003). Tuning experimental parameters is common practice for reducing adduct formation and fragmentation in the LC-MS step (Ardrey 2003, p. 232). However, in untargeted metabolomics, where a wide range of metabolites with diverse physico-chemical characteristics are profiled simultaneously, it is difficult to find a set of parameters that globally corrects for adduct formation and in-source fragmentation (Ardrey 2003 p. 232). Adducts and fragment ions can result in the same metabolite being redundantly represented in the dataset (Klåvus et al. 2020), although these can, to the degree that they can be identified in a reference database, be combined into a single entry before obtaining the quantitative dataset using software such as MS-DIAL (Tsugawa et al. 2015). This reduces unwanted experimental variation (Broeckling et al. 2014), multicollinearity and problems with identification (Klåvus et al. 2020). Sources of measurement error include heteroscedastic noise (Berg et al. 2006), spectral skewing (Berg et al. 2006) and drift in the relative abundance of features as evidenced by QC samples, which can be caused by many factors in the instrumentation and non-enzymatic metabolite conversion (Broadhurst et al. 2018). An important source of unwanted biological variation is the differential dilution of samples (Dieterle et al. 2006). Another common occurrence is missing values, often because of a concentration below the limit of detection, errors in preprocessing of the data or stochastic shifts in instrument function (Wei et al. 2018).

## Data pretreatment

In data pretreatment, unwanted variation is reduced with computational methods and quality is assured with QC before preparing the dataset for feature selection. There is some discussion in the literature on the use cases in which data pretreatment methods are best applied, which is dependent on the feature selection methodology. In Notame, the imputed and clustered dataset is used without any transformation, normalization or scaling for univariate analysis after QC. For supervised learning, nlog or glog transformation, probabilistic quotient normalization (PQN) and autoscaling is used, although autoscaling is not needed for scale-invariant methods such as random forest.

Though specifying data pretreatment, Notame partially omits references instilling confidence in the practices, where different combinations of pretreatment methods may yield different results. Guida et al. (2016) systematically investigated the effect of 280 permutations of normalization, imputation, transformation and scaling methods after QC, omitting feature clustering and drift correction. Conclusions were arrived at by splitting a dataset into two groups with no significant differences and then modifying the peaks of one group to reflect significant differences and non-significant differences. This modified dataset was then used to evaluate permutations of data pretreatment performed for univariate tests and partial least squares discriminant analysis (PLS-DA) using the known true positives (significant differences) and false positives (non-significant differences) in the modified dataset. The results indicate that univariate analysis is best undertaken with a non-imputed dataset normalized using PQN. For PLS-DA, the results suggest using k-nearest neighbors imputation and glog transformation.

Then there is the order in which the pretreatment is performed. In Notame, features are first normalized for drift, followed by QC, imputation and clustering. The data is then transformed, normalized for dilution and scaled depending on the feature selection conducted. There is no discussion in the literature on the order in which the data pretreatment steps are best applied.

### Normalization
In the LC-MS literature, normalization mostly refers to reducing feature-wise or sample-wise unwanted variation, namely from from drift and dilution. LC-MS measurements suffer from systematic intensity drift resulting in a change in abundance as a function of injection sequence, constituting systematic measurement error (Broadhurst et al. 2018). The removal of drift increases the quality of the data by reducing variation introduced by the systematic measurement error while conserving the biological variation of interest (Märtens et al. 2023). The simplest drift correction method is scaling to total feature abundance, median abundance or some other metric calculated for the dataset or study groups/time point at whole (Märtens et al. 2023). These integral normalization methods, however, assume that all metabolites experience the same pattern of variation (Märtens et al. 2023) and that there is no unwanted biological variation from dilution. Commonly, isotope-labeled internal standards included at a uniform concentration are used to correct for drift, but this isn't feasible in untargeted approahces since isotopes for thousands of metabolites would have to be inluded (Sysi-Aho et al. 2007). Computational techniques have been devised to select a subset of optimal standards to correct drift for all features (Sysi-Aho et al. 2007). Drift can also be countered by real-time correction, where the detector voltage is calibrated after each sample (Lewis et al. 2016). This is unlikely to account for all sources of drift, for example non-enzymatic metabolite conversion (Broadhurst et al. 2018).

The most widely used drift correction method in the literature is QC-based drift correction (Broadhurst et al. 2018), which is used in Notame. Pooled QC samples included at regular intervals in data collection, consisting of aliquots from each sample, are used to remove the drift using a predictive model fitted to the features in QC samples by injection order (Broadhurst et al. 2018).

In Notame, univariate regression is used to model the relationship between the independent, explanatory variable (injection order) and the continous, dependent outcome variable (abundance). Univariate regression models the relationship by fitting a line that best fits the data, approximating the true relationship between the explanatory variable and the outcome variable. The approximation is evaluated by the residual, that is the distance between the value of the outcome variable to the modeled value of the outcome variable for a given value of the explanatory variable. Many assumptions are made in univariate regression, although these are likely to be violated in metabolomics datasets despite transformation because of the large number of features. Then again, the quality metrics used to evaluate and flag features in QC are not necessarily dependent on the assumptions as non-parametric equivalents are available.

Relaxing the linarity assumption, one can use a non-linear regression model, such as the smoothed cubic spline used in Notame. The data is nlog-transformed to better meet the assumptions of homoscedasticity and normality of residuals. The cubic spline is then used to model each feature's abundance across QC samples by connecting the observations using third order polynomials. Since QC samples are included at regular intervals interspersed with biological samples, the smoothing serves to interpolate the model to the biological samples (Broadhurst et al. 2018). The value of the smoothing parameter is optimized using leave-one-out cross validation to avoid overfitting to unwanted variation in the QC samples (Broadhurst et al. 2018). Having fit the smoothed cubic spline regression model, abundance values are corrected by adding the mean of a feature's abundance in the QC samples and subtracting the drift for the feature in each sample (Klåvus et al. 2020).

Another central source of unwanted variation is the differential dilution of samples, resulting in varying total feature abundances across samples (Dieterle et al. 2006). Normalization can reduce unwanted variation from dilution while retaining the biological variation of interest (Dieterle et al. 2006). Dilution, in this context, can refer to both experimental variation and unwanted biological variation. For example, in normalization for dilution of urine samples, each feature can be scaled to the abundance of creatinine (Waikar et al. 2010). This integral normalization method rests on the assumption that dilution is the sole source of unwanted variation and that the excretion of creatinine can be used as a robust, constant scaling factor across samples (Dieterle et al. 2006). Another widely used method is quantile normalization. In quantile normalization each sample is given the same distribution of feature abundances (Kohl et al. 2012). The resulting feature abundances in a sample consist of the same set of values, but they are distributed differently among features (Kohl et al. 2012). Quantile normalization is problematic as a feature may end up with the same abundance across all samples, especially in the case of low- and high-abundance features (Jauhiainen et al. 2014).

The above methods do not take into account different sources of variation, which is important if it can't be reasonably assumed that the variation in total feature abundance arises predominantly from dilution as in the case of urine (Dieterle et al. 2006). In many biological samples, biological variation of interest has an undeniable effect on the total abundance of features in a sample, in which case integral normalization not only scales for differential dilution of samples, but also for the biological variation of interest (Dieterle et al. 2006).

PQN has emerged as a promising general approach, and is used in Notame. PQN posits that changes in metabolite concentrations only influence some features, whereas differential dilution influences the whole sample and that dilution is the main source of reducible unwanted variation (Dieterle et al. 2006). To accommodate biological variation and variation from dilution, PQN determines a dilution factor by calculating a distribution of quotients from the median of each feature in the biological samples and a reference spectrum (Dieterle et al. 2006). The median of these quotients is used as a dilution factor for all samples (Dieterle et al. 2006). The best reference spectrum is the median abundance of each feature across QC samples (Dieterle et al. 2006).

When normalizing for dilution between batches, an integral normalization can be performed before PQN to scale the batches to the same total abundance (Dieterle et al. 2006). PQN can also be performed on the raw data if several batches will be preprocessed as a single batch (Dieterle et al. 2006). Alternatively, identical long-term reference pooled QC samples can be used to normalize for dilution between batches (Broadhurst et al. 2018). Standard reference materials also hold promise in such batch normalization (Broadhurst et al. 2018) and even absolute quantification, although this is a new development that suffers from having to include internal standards for each metabolite or computationally selecting a subset of standards.

### Quality control
Including multiple pooled QC samples allows for calculation of measures of
precision for each metabolite detected in the QC samples. These calculations can also be made for many batches with long-term reference QC samples (Broadhurst el al. 2018). Absolute estimates of accuracy, that is how close the measured concentration is to the true concentration, and absolute estimates of precision, that is random measurement error and experimental variation, in quantification over repeated measureument of identical samples can not be obtained using untargeted LC-MS (Broadhurst et al. 2018). This is again because of the difficulty in using internal standards for limits of detection, limits of quantification, and linearity quantifiers for each feature (Broadhurst et al. 2018). Thus three metrics, the relative standard deviation (RSD), the dispersion ratio (D-ratio) and the detection rate are widely used for QC of untargeted metabolomics data (Broadhurst et al. 2018), also in Notame.

Only the relative random measurement error in quantification of identical QC samples can be obtained as a measure of precision in untargeted LC-MS metabolomics data analysis (Broadhurst et al. 2018). If the random measurement error across identical QC samples is represented as a normal distribution, the random measurement error can be described in terms of standard deviations for each feature (Broadhurst et al. 2018). To compare the standard deviation of the features, the RSD is calculated for each feature by dividing the standard deviation with the mean of the feature (Broadhurst et al. 2018). The non-parametric median absolute deviation can also be used for such calculations, which can be scaled for comparison to RSD using the scaling factor 1.4826 (Broadhurst et al. 2018).

A measure of total experimental variation and measurement error for each feature is obtained by dividing the standard deviation of the identical QC samples with the standard deviation of the biological samples (Broadhurst et al. 2018). The result is the D-ratio, for which a non-parametric alternative is also available (Broadhurst et al. 2018). The standard deviation of features in the identical pooled QC samples should represent only experimental variation and random measurement error, while the biological samples include experimental variation, random measurement error and biological variation (Broadhurst et al. 2018). Assuming an additive random error structure, the total variation can be simplified to experimental variation and random measurement error plus biological variation (Broadhurst et al. 2018). From this it can be concluded that a D-ratio of 0 means that there is no experimental variation or random measurement error in the feature, and only the biological variation of interest is conserved. On the other hand, a D-ratio of 1 translates to that there is only experimental variation and random measurement error, with no biological variation. The assumption that the random error structure is additive may not hold; it is commonly known that random measurement error is multiplicative as it results in heteroscedasticity (Veselkov et al. 2011), although this can be mitigated with transformation.

Finally, there is the detection rate. The detection rate is derived by dividing the observed number of each feature across QC samples with the total number of QC samples (Broadhurst et al. 2018). Features with a low detection rate are not reliable (Broadhurst et al. 2018), and probably arise because of the detection threshold. These three metrics (RSD, D-ratio, and detection rate) provide an assessment of quality that can be reported for each detected metabolite and are used to remove low quality data from the dataset prior to further data pretreatment and analysis. The exclusion criteria for the quality metrics are not set in stone. It may also be motivated to use two sets of exclusion criteria, for example to prevent removal of features with very low values in all but a few samples (Kärkkäinen et al. 2021). Notame recommends a detection rate > 0.7, RSD < 0.2 and D-ratio < 0.4.

Because of the large number of features, it is not feasible to inspect feature-wise plots at this stage of the analysis. Visual inspection is performed for each mode separately. This also serves as exploratory data analysis to form intuitions about the dataset at hand. An overview of the literature suggests that the Notame QC visualizations are quite comprehensive and somewhat original; further review is thus omitted.

### Missing value imputation

Many downstream methods rely on a complete dataset with no missing values. Values missing due to being below an instrument’s limit of detection are often referred to as missing not at random (MNAR) (Dekermanjian et al. 2022). Missing values caused by, for example, incomplete ionization because of stochastic shifts in instrument function are often referred to as missing completely at random (MCAR) (Wei et al. 2018), and are uniformly distributed in the dataset. Missingness which is dependent on other features in the dataset is called missing at random (MAR), and often arise from errors in preprocessing (Wei et al. 2018). MARs and MCARs are often indistinguishable in practice (Dekermanjian et al. 2022). Notame deals with much of the missingness by removing features that are not detected in > 70% of the samples, removing features that are at the detection threshold to a greater extent as such features also include MNARs. If any MNARs from around the detection threshold remain in the dataset, they may be imputed as if they were MCARs/MARs by the random forest method used in Notame (Wei et al. 2018). This is not optimal, as it has been found that other methods are better suited for imputing MNARs (Wei et al. 2018). Classification of missing values as MNARs and MCARs/MARs has been used to separately and more accurately impute missing values arising from different missingness mechanisms (Wei et al. 2018, Dekermanjian et al. 2022). Random forest imputation has been shown to outperform other methods in imputing MCARs/MARs (Kokla et al. 2019).

Removing features with missing values altogether is one way to deal with missing values, although this would result in a severely diminished dataset against the spirit of untargeted LC-MS metabolomics research (Wei et al. 2018). Removing samples with missing values is likely to result in an even more diminished dataset due to the the high number of features compared to samples, and reduced certainty of findings (Dekermanjian et al. 2022).

### Clustering features originating from the same metabolite
MS-DIAL combines many isotopes, common adducts and in-source fragments into a single feature (Tsugawa et al. 2015). Still, redundant representation of the same metabolite can not be ruled out (Klåvus et al. 2020). Redundancy in the dataset can be reduced by RT-based clustering, based on the intuition that metabolite species reflect the abundance of the parent metabolite across samples and have similar RTs (Klåvus et al. 2020). This is done separately for each mode in the dataset due to differing RTs.

In Notame, features are clustered based on correlated feature pairs within a specified RT window and correlation threshold. RT-based clustering is an advancement in relation to general clustering methods such as k-means clustering. A handful of RT-based clustering methods have been described (Broeckling et al. 2014, Joo et al. 2023), but further review is omitted as they are not as established as earlier data pretreatment steps.

The Notame method first identifies pairs of correlated features within a specified RT window, and features that don't meet a specified correlation coefficient threshold are considered groups of their own. The intuition for this is that co-eluted features with similar RTs and correlated abundances are likely to originate from the same metabolite. Notame uses Pearson's correlation for correlating the abundances, assuming that the relationship of features originating from the same metabolite is linear: the co-eluted feature abundances increase as a linear function of each other. This is not necessarily the case, especially at the lower and upper limit of quantification of the instrument (Klåvus et al. 2020).

Next, an undirected graph is generated using feature correlations as edge weights. Connected nodes are then treated as separate groups in a recursive algorithm, where features originating from the same metabolite are identified using a degree threshold. This serves to reduce the number of false annotations in a situation where a feature is connected to only one or two nodes in the group, and are unlikely to originate from the same metabolite despite co-elution. The degree threshold is defined as a percentage of the maximum possible degree, where degree is the maximum number of connections from a single node. Thus, for a group with five nodes, the degree is 4 and a degree threshold of 0.8 would mean that each node must have at least 3 edges (0.8 x 4 = 3.2 ≈ 3). Until this criterion is met in each group, the node with the lowest degree is discarded, or if there is a tie, the node with the lowest sum of edge weights. The result is a cluster where each node is connected to all other nodes, reflecting co-elution and abundance correlation because they originate from the same metabolite. The nodes that are initially discarded can form new clusters if the degree criterion if fulfilled. The feature with the largest median abundance is retained for each cluster.

After feature clustering, the datasets corresponding to different chromatographic modes are merged. This results in a dataset with some redundancy as many features are detected in multiple modes.


### Transformation and scaling

Transformation and scaling completely change the feature abundance space, in contrast to normalization for drift and dilution. Multiplicative random measurement error and spectral skewing introduces heteroscedasticity and skewness in the features, respectively (Berg et al. 2006). Transformations are common for reducing heteroscedasticity and skewness to better meet the assumptions of statistical analyses (Berg et al. 2006). Moreover, in metabolomics, relations among metabolites may not always be additive, and log-transformed values can better account for multiplicative relations with linear techniques (Berg et al. 2006). A log10 transformation perfectly removes heteroscedasticity if the random measurement error increases linearly with abundance; the RSD is constant (Kvalheim et al. 1994). However, low-abundance features typically have a larger RSDs from experimental variation (Berg et al. 2006). For such non-linear increase in multiplicative random measurement error, the power transformation can be used, although this does not perfectly remove heteroscedasticity or promote conversion of multiplicative to additive relations (Berg et al. 2006). In Notame, a natural log transformation or glog transformation, in case of heavily skewed data, is used. These two options were arrived at through the comparative investigation by Guida et al. (2016), although no more discussion is available in the literature.

Another challenge in LC-MS data is that different metabolites can have very large differences in abundance. These up to 5000-fold differences in abundance do not reflect the biological importance of metabolites (Berg et al. 2006). Instead, much insight is extracted from how the intra-feature variation compares to that of other features. To make the intra-feature variation comparable across features, the variation needs to be represented on the same scale. This can be done by scaling, for example using the standard deviation of each feature as a scaling factor (Berg et al. 2006). Mean centering followed by division of abundances with the standard deviation, autoscaling, results in features having the mean at zero and a standard deviation of one, a prerequisite for many machine learning methods based on distance measures. On the other hand, scaling results in inflation of small abundance features (Berg et al. 2006). Other scaling methods, such as pareto scaling, range scaling and vast scaling were compared in conjunction with other data pretreatment steps by Berg et al. (2006), Guida et al. (2016) and Kohl et al. (2012). Autoscaling and pareto scaling were deemed most promising. Transformation also scales the features somewhat as the difference between large and small abundance features is reduced, but this does not bring the features to the same scale (Berg et al. 2006). Notame specifies autoscaling.



## Feature selection

Feature selection aims to rank features with regards to study group/time point to select a subset of features for downstream methods pertaining to biological context. DAWG has yet to put forth guidelines for reporting of how features are selected for pathway analysis, annotation and other steps which provides biological context for the findings. These steps are often labour intensive: for example, database matching represents only a putative metabolite annotation that must be confirmed by comparing the RT and/or LC-MS/MS spectra of a pure compound to that from the feature of interest (Vinaixa et al. 2012). This is time consuming and represents is the rate-limiting step (Vinaixa et al. 2012). In Notame, feature-wise and comprehensive visualization of results is used to facilitate and communicate intuitions about interesting features instead of just presenting the results in a boring table. The visualizations used inevitably shape the interpretation of the results. The literature suggests that the Notame visualizations are comprehensive, although many additional visualizations could be used to visualize the properties of interest, for example bar plots, swarm plots, violin plots and Euler diagrams. One feature-wise perspective omitted in Notame is reinspecting the raw spectra to assess the validity of the results (Grace and Hudson 2016). This could reveal falsely high-ranked features caused by scaling artifacts or spurious peak assignment (Grace and Hudson 2016). Notame includes three plots for relating RT, m/z and feature selection results which could be combined into a single cloud plot such as the one available in XCMS Online to facilitate interpretation (Patti et al. 2013).


### Univariate analysis

Univariate, that is feature-wise, analysis can be divided into parametric and non-parametric methods. Parametric methods are based on assumptions about the population from which the samples were drawn, such that the data can be described by a mean and standard deviation. Assumptions do not burden non-parametric methods to the same degree. Some methods relax an assumption for the study design at hand, for example in repeated measures designs where the assumption of independence can be accommodated in a linear mixed effects model. Parametric and non-parametric methods differ in using the mean and median or ranks of the distribution, respectively. Parametric methods are more powerful: a non-parametric method can miss a statistically significant difference that a parametric method would recognize if assumptions are met (Vinaixa et al. 2012). With non-normal distributions, heteroscedasticity and unequal study group/time point sizes, non-parametric methods are preferred (Vinaixa et al. 2012). There are tests for assessing whether an assumption is met, although the conceptual underpinnings and interpretation are debatable. For example, normality tests aim to assess what the probability of the null hypothesis, namely that the values in the population are normally distributed, being the case on chance. With a larger population, more stringent requirements for conformity with normality are required in normality tests such that large populations seldom meet the assumption of normality, although large populations approach normality as per the central limit theorem (Vinaixa et al. 2012). For real stringent testing of assumptions, multiple testing correction may be considered, which is likely to result in the rejection of the null hypothesis in testing for normality and homoscedasticity for a large number of features. Whether or not multiple testing correction is used, the literature suggests that features not meeting some assumptions are usually not removed in metabolomics studies; instead parametric tests are often used by precedent. Parametric tests are robust against mild violations of assumptions, but what is considered mild depends on the context.

Given the large number of features in untargeted approaches and that either a parametric or non-parametric approach should solely be used, violations of assumptions of parametric tests, the reduced power of non-parametric tests or removal of features not meeting assumptions must be accepted. Testing for normality and homoscedasticity using the Shapiro-Wilk and Levene's tests, around 40-80% of features in LC-MS datasets seem to meet the assumption of normality and homoscedasticity without transformation (Vinaixa et al. 2012), as is suggested for univariate analysis in Notame.

The probability threshold for rejection of the null hypothesis is, by convention, 5%, and can also be thought of as the probability of obtaining a false positive result. A p-value larger than 0.05 can thus be thought of as a failure to reject the null hypothesis due to lack of evidence. However, in univariate analysis for feature selection, the interpretation of statistical significance is not critical if the p-values are solely used to rank features to find a subset of interesting features, illustrating the context-dependent nature of the probability threshold.

Fold-change of a feature, a commonly used measure of effect size in metabolomics, and statistical significance do not account for sample size, so power analysis is often used to validate findings (Vinaixa et al. 2012). Post-hoc power analysis, however, is considered fraught with conceptual problems (Vinaixa et al. 2012), and a priori sample size estimation is problematic because of the highly multicollinear data (Hendriks et al. 2011). Accordingly, there is no mention of statistical power in Notame. A pilot study could be conducted to estimate the number of samples needed for an untargeted LC-MS study to make a convincing case for univariate results (Iterson et al. 2009). Then again, ethical and economical considerations mainly determine the number of samples (Vinaixa et al. 2012), especially given the ambiguity of power analysis and the labour-intensive nature of untargeted approaches.

Notame proposes the following univariate analyses, covering a range of study designs. For case versus control studies with two groups and no covariates, Welch’s t-test is used as it allows for unequal variances between groups. The Mann-Whitney U-test can be used as a non-parametric alternative.

For study designs with multiple groups, Welch’s ANOVA, which allows for unequal variances, is used to select interesting features based on p-values. Welch's ANOVA is quite resistant to non-normal distributions. Heteroscedasticity is a problem of similar magnitude for the non-parametric alternative, the Kruskal-Wallis test, and isn't specified by Notame. To investigate differences between multiple groups post-hoc, pairwise Welch’s t-test or Mann-Whitney U-test is used post-hoc.

In the case of two categorical study factors, two-way ANOVA is applied to examine the main effect of each factor and their interaction. If factors have multiple levels, interesting features are selected based on overall p-values and further examined using Welch’s t-test. Friedman test can be used as a non-parametric alternative to two-way ANOVA, with Mann-Whitney U-test for post-hoc comparisons.

For repeated measures designs, a linear mixed effects model is used with the time point, group and interaction factors as fixed effects and subjects as random effect. To assess the significance of effects between no more than two groups or time points, t-tests are applied on the regression coefficients of the fixed effects. If multiple groups and/or time points are included, type III ANOVA is used to assess the significance of the effects, returning p-values from an F-test.

To investigate the association between features or between features and other variables, Pearson correlation or the non-parametric Spearman correlation is used.

Finally, p-values are adjusted using the Benjamin-Hochberg false discovery rate (FDR) approach for multiple testing. Univariate analysis does not suffer from sparsity typical of biomolecular data as only one feature is considered at a time. However, the large number of features considered increases the likelyhood of false positives if univariate analysis are used for hypothesis testing. Multiple testing correction is done to correct for the chance of obtaining significant results in a situation where, on average, a significance threshold of 0.05 for twenty tests would give a false positive for one test (Jafari et al. 2019).


### Supervised learning

Complex, biological systems are multivariate by nature: they can't be described in terms of single variables, but can be approached with modelling multiple variables in the interconnected whole (Goodacre et al. 2007). However, multivariate analysis in molecular biology is complicated by "the curse of dimensionality", or sparsity: datasets have many features but relatively few samples. Adding features in a dataset results in an exponential volume increase. For example, evenly sampling 10^2=100 values between 0 and 1 would result in a vector with a spacing of 0.01 between points. To arrive at the same spacing in a dataset with ten features, 10^20 observations would be needed. The number of features in the dataset is somewhat reduced by removal of low-quality features and feature clustering, but multivariate analyses still suffer from sparsity of the data. In practice, the curse of dimensionality is encountered when the performance of a multivariate method starts to deteriorate as the number of features increase, given constant sample number. This is referred to as overfitting. Selecting a subset of informative features (herein referred to as variable selection) and validation of the model can mitigate overfitting.

Supervised learning is widely used in molecular biology, where the data is modeled with respect to specific outcome variables, namely study group/time point. Some supervised learning methods, like Lasso regression, are well suited for sparse data since they perform embedded variable selection; variable selection is inherent to the method (Rinaudo et al. 2016). Other supervised learning methods, such as random forest and PLS-DA, often employ wrapper methods where the importance of features are assessed and selected when training the model using cross-validation or bootstrapping (Rinaudo et al. 2016). Some sources suggest that results from univariate analysis could be used for variable selection, constituting a filter method (Xia et al. 2013). However, filtering features based on results from univariate analysis doesn't reflect the intuition behind supervised learning. Univariate analysis, as noted in the previous section, is concerned with whether a difference between study groups/time points is due to chance, which doesn't translate to classification performance, which is the concern of supervised learning methods used for feature selection (Xia et al. 2013). Moreover, multicollinear features can be significant in aggregate, but aren't necessarily found so by univariate analysis (Xia et al. 2013), which may cause a filter method to eliminate informative features. Multicollinearity can be accounted for using a filter method by mutual information criterion (Lin et al. 2012) or correlation coefficient (Grissa et al. 2016).

Validation of supervised learning models is done by training on a subset of the dataset, the training set, after which the model is validated on a validation set (Xia et al. 2013). This is often done in a cross-validation scheme, where training and validation sets are repeatedly subset from the dataset so that the model doesn't overfit to a particular subset of the data (Xia et al. 2013). A further held-out test set is used to evaluate the performance of the model on data not used in training and validating the model (Xia et al. 2013). This serves as an estimate as to how well the model generalizes to new data.

Notame specifies feature selection by supervised learning to be performed using random forest or PLS-DA, preferentially using the MUVR package, featuring unbiased variable selection (Shi et al. 2019). Selection bias is introduced when features are selected based on all or some of the data used in training the model. MUVR minimizes selection bias by repeatedly and randomly sampling the entire dataset to a training/validation set and a test set and averaging the feature ranking across such outer repetitions. For each outer repetition, a validation set is randomly sampled from the training set to select the optimum model parameters from models fit in a cross-validation scheme on the remaining training set. A proportion of low-ranking features are eliminated, and optimum model parameters and average feature ranking is obtained by repeating model fitting and validation on the reduced and anew randomly sampled training and validation set. The optimum model is then trained on a combined validation and training set, used to obtain the optimum model for feature ranking using the test set in the given outer repetition. With several outer repetitions, this method arrives at optimally ranked features and model parameters for the entire dataset. The above is for relative simplicity, as MUVR actually returns a "min", "mid" and "max" model tailored to the analytical problem. The "min" model returns likely biomarker candidates, while the "max" model returns a set of features which encompass all the information content, used for pathway analysis. The "mid" model is a compromise between the "min" and "max" models, and is likely to perform best for classification in replicate experiments or diagnostic applications, for example. Finally, MUVR also allows for multilevel classification, which is handy for classification of both study group and time point membership.


### Feature ranking

In Notame, feature ranking from univariate analyses and supervised learning are combined to determine the most biologically relevant features for identification. First, the ranks from supervised learning are sorted such that the most predictive feature comes first. Univariate results are sorted similarly based on the p-values. The univariate and supervised ranks are then summed and sorted to create a combined ranking. The number of ranked features to be explored for biological meaning depends on the user.

Such combined ranking is not showcased in any likely candidate papers, and the intuition is not elaborated in Notame. Combining results from univariate analysis and supervised learning in the combined ranking  constitutes a combination of hypothesis testing and the importance for classification. This also brings the assumptions of univariate analysis and variability of results in supervised learning into the picture. Other sources recommend using both univariate and multivariate analyses for feature selection to maximize the extraction of relevant information, but approaches these as separate findings (Vinaixa et al. 2012).


## Biological context

The above steps in LC-MS metabolomics data analysis are used in some form in most of metabolomics research to select interesting metabolites across study groups/time points for discovery of biomarkers and the mechanistics of biochemical pathways, perhaps with increased leverage from other omics and clinical data. Univariate analyses in untargeted approaches support the above ends via selection of features for further scrutiny as in Notame, but hypothesis testing is often not an end in itself. Targeted approaches are better equipped to confirm tentative results from untargeted analyses with more specific hypotheses pertaining to differential metabolite abundance across study groups/time points because limits of detection, limits of quantification and linearity quantifiers can be addressed using standard reference materials (Broadhurst et al. 2018). As such, the quality and reproducibility of untargeted research mainly has implications for directing effort in further targeted research (Johnson et al. 2016). This somewhat lessens quality concerns in untargeted approaches, as insight gained by untargeted approaches is likely further investigated by targeted approaches before scientific consensus and application (Johnson et al. 2016).


### Annotation

Annotation aims to identify metabolites of interest and record their physico-chemical propertires, which can prompt discussion on how the results relate to the biochemical workings of the biological system at large. MSI has set forth three levels for metabolite identification (Sumner et al. 2007). Metabolites annotated by matching m/z, LC-MS/MS spectra and RT to a reference database are considered identified. Metabolites with matching m/z and MS/MS spectra are considered putatively identified if the MS/MS spectra uniquely matches a reference metabolite. Finally, putative characterization status is given to metabolites for which only compound class can be established.

Annotation has not been fully automated although there are advances. Annotation often proceeds first with an automated step, often in point-and-click software as per Notame, followed by manual curation. Database quality is paramount in annotation. Annotation is limited by known unknowns, as only a subset of metabolites are included in databases, although some physico-chemical properties can still often be assigned (Zulfigar et al. 2023). Another issue is the relatively poor reproducibility of MS/MS spectra, affecting the two higher quality metabolite identification standards put forth by MSI (Zulfigar et al. 2023).


### Pathway analysis

Pathway analysis puts the results in a mechanistic context, providing insight into the biochemical processes across study groups/time points. This can inspire further research into disease mechanisms, for example. Pathway analysis can also be performed in a multi-omic fashion for increased leverage. Notame recommends pathway analysis to be conducted with at least putatively annotated metabolites. Pathway analysis, in addition to preprocessing and annotation, is performed with point-and-click software in Notame.


### Multi-omics

Although not discussed in the Notame protocol article, multi-omics expands on the promise of metabolomics in teasing apart the complexities of biological systems, for example in complex disease. Multi-omics is promising in that the variables are molecule abundances, which can generate hypotheses for further mechanistic research and treatments.

Multi-omics makes for even sparser data. Multi-omics research often utilizes supervised or unsupervised learning. There are several strategies for integrating omics datasets for multi-omics supervised learning. The late integration strategy considers interesting features found in separate analyses of each omics dataset (Picard et al. 2021). For late integration, features could be selected from the combined ranking or multivariate ranking (Picard et al. 2021). Depending on the supervised learning methods applied, some of the other multi-omics integration strategies could in principle integrate data without pretreatment (Picard et al. 2021), but many of the above datapretreatment steps are likely to benefit almost any strategy.

Complex disease etiology can't, by definition, be reduced to simple mechanisms as in, for example, monogenic diseases. Similar clinical phenotypes may also be lumped under the same disease classification, despite possibly exhibiting different etiologies. Moreover, disease classifications are often based on spurious historical circumstance relating to social and economical factors (Scully 2004) and symptomaeology (Johansson et al. 2023). Doing research on such disease definitions may not be optimal for development of diagnostics and treatments (Johansson et al. 2023). Unsupervised learning, especially in a multi-omics context, may advance the understanding of how complex disease phenotypes arise (Johansson et al. 2023). Disease conceptualization is a central topic in philosophy of disease. Realists posit that diagnoses are natural kinds, fundamental units of inference (Watson 2023). Given the complex nature of diseases such as diabetes, a more nuanced approach is to use data from currently available experimental technologies to pragmatically reconceptualize the disease to further health outcomes. This approach views diseases as constructed groupings of unwell people (Watson et al. 2023). The question then becomes: how do we best group patients to improve their health? Ethics aside, sample clustering methods can help in answering the above question by integrating omics data, clinical data such as treatment response and other data to classify patients into disease subtypes (Johansson et al. 2023). The late integration strategy is not suited for sample clustering in multi-omics, as sample clusters would constitute separate findings across omics modalities. Instead, sample clustering is often preceded by PCA, factor analysis, autoencoders or some other unsupervised dimensionality reduction method (Picard et al. 2021). The disease subtypes or wholly new conceptualizations of disease can then be used to generate new hypotheses for mechanistic research and treatments (Johansson et al. 2023).


## Reproducibility

Reproducibility of scientific results is implicit in the science: research credibility hinges on being able to reproduce findings (Fidler et al. 2021). This requires capturing all information relevant for reproducing the results, that is provenance (Kanwal et al. 2021). The starting point for reproducing research may vary due to practical reasons. In research involving substantial computation, the notion of reproducibility is often restricted to the computations alone (Fidler et al. 2021). This is practical from the perspective of open science, as reproduction of computational results can often be done on a personal computer provided sufficient provenance.

As such, raw data is a natural starting point for reproducing of research involving computation. Indeed, "no raw data, no science" is a slogan of sorts in addressing reproducibility issues (Miyakawa et al. 2020). The editor of Molecular Brain found that upon demanding raw data to be made available, over half of submitted manuscripts are withdrawn from the publication process (Miyakawa et al. 2020). Making raw data available may limit malpractice, arising from incentives which do not always align with scientific rigor. Omission of data may be due to ethical considerations, but given public funding of research and the resolution of technical constraints, the limited availability of raw data is largely unfounded (Miyakawa et al. 2020). This was underlined by an unsuccessful attempt to find raw data with specifications suitable for this thesis. Making raw data available may support the development of best practices (Goodacre et al. 2007).

Even with sufficient provenance, analyses may not be exactly reproducible. Even using the same seed, slightly different results can be observed. Such discrepancies may result from instability of computational methods, for example embedded variable selection (Rinaudo et al. 2016).

Reproducibility supports the broader notion of replicability, denoting substantiation of single studies by triangulation from a growing literature, which may include other experimental approaches. It has been argued that the so-called replication crisis is to a large part due to overconfidence in statistical results (Amrhein et al. 2019): all models are wrong, but some are useful. Scientific generalizations need to be based on cumulative knowledge rather than on a single study, the replicability of which can be improved by quality standards (Fidler et al. 2021), perhaps best practices. As such, replicability may be thought of as research quality. One source of replication problems is the multiplicity of data analysis strategies, where different conclusions can be drawn from the same data depending on the computational methods applied (Fidler et al. 2021). This epistemic uncertainty is not addressed by uncertainty metrics like confidence intervals which apply in the context of specific methods (Hoffmann et al. 2021). In the context of supervised learning, many models may have similar performance, but yield different interpretations, for example feature importance scores. This is called the Rashomon effect (Breiman 2001). There are several approaches to addressing the variability of results, although it seems like such methods have not been adopted in metabolomics. In other disciplines methods to address the variability of results include specification curve analysis, multimodel ensembles and bayesian model averaging (Hoffmann et al. 2021).

### Bioconductor

Bioconductor is a centralized repository for high-quality open research software, which can be conceptualized as consisting of data containers, computational methods and a community of users and developers. Interoperability is a of central focus in Bioconductor: computational methods are preferrably contributed such that they can be used flexibly according to different research needs (Gentleman et al. 2004), like Lego bricks. In Bioconductor, the interoperability continuum is apparent in package functions not relying on a rigid pipeline of functionality and the manipulation of data using data containers. Using the same data container is more interoperable than having to separate and/or create new data structures in an analysis workflow. Given the emphasis on interoperability in Bioconductor, redundancy of functionality is discouraged in Bioconductor.

Software compatibility underlies interoperability, and is largely concerned with dependencies. Dependencies may be thought of as code from other parties required to make code work correctly. Managing dependencies is not a trivial task, and may result in frustration referred to as "dependency hell". Bioconductor delivers releases consisting of a set of compatible R packages intended for compatibility within a certain version of R (Gentleman et al. 2004). This makes analyses more provenant, as simply the R version and Bioconductor packages used can suffice with regard to reporting the software used for the analysis. There is also a quality aspect to using Bioconductor: the Bioconductor team manually tests the packages and there are requirements for documentation and testing of functionality (Gentleman et al. 2004).

When manipulating complex data, it needs to be abstracted. Abstraction, in general, is the process of reducing something to a set of essential elements. Data containers meet this description in simplifying the representation of data, while hiding its complexities and associated operations (Morgan et al. 2023). For example, instead of storing feature and sample metadata in separate tables and accessed by extensive scripting, metadata is stored in the same container instance and accessed by user-friendly operations. Bioinformatics projects typically require a data container with a count matrix, sample descriptions and annotation functionality (Morgan et al. 2023). Data containers make functionality more interoperable and easier to grasp for readers as compared to extensive scripting, promoting reproduction of the analysis. This is especially true if a single container is used for the analysis. Container converters are handy if several data containers are needed in the analysis. Data containers are also useful for comprehensive reporting of results, uncertainty and experimental details in a single instance. For example, metrics pertaining to assumptions of univariate tests can be included in the feature metadata for inspection of high-ranking features.


### Quarto

Quarto is an open-source computational document system which generates an output file of desired format based on text and code input (Allaire et al. 2022). Provenance is improved using Quarto, as figures and results can output directly to the rendered document.

Much of the proposed minimum reporting standards from DAWG are covered by the text and code in a Quarto document, especially with informative commenting of code. The reproducibility advantages of computational document systems have been acknowledged by the scientific community, for example in Bioconductor where use of computational documents is required for documentation. IMoreover, the eLife journal accepts such so-called executable research articles. This is in stark contrast especially to results arrived at using point-and-click software, where the analysis is often not recorded and results supposedly gained as per the description of the analysis are accepted on good faith (Kanwal et al. 2017). Technologies have been developed to record the analyses performed using point-and-click software in formats which can be run from the command line, although these often present limitations as to changing of parameters, for example (Kanwal et al. 2017).

As such, it would be optimal to implement workflows fully programmatically, allowing for readers to easily reproduce the results tinker with the analysis to assess the variability of results. Using Quarto and Bioconductor packages for the analysis makes this prospect especially tractable as simply the Bioconductor version and packages can be stated. This thesis itself can be generated in a streamlined manner and is amenable to users with minimal programming experience. Moreover, providing the code in a computational document allows for flexible re-use of code, further facilitating development and dissemination of best practices.

Other technologies which increase the reproducibility of analyses include virtual environments and containerization software (Kanwal et al. 2017). Virtual environments facilitate the recording and sharing of dependencies. Containerization software additionally provides a stable machine image, which counters platform-specific variability of results (Kanwal et al. 2017).
